{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afc2fb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mzc01-hyucksangcho/opt/anaconda3/envs/pyflink_38/lib/python3.8/site-packages/pyflink\n"
     ]
    }
   ],
   "source": [
    "# Pyflink 저장 위치\n",
    "import pyflink\n",
    "import os\n",
    "print(os.path.dirname(os.path.abspath(pyflink.__file__)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175544e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyflink.common.configuration.Configuration at 0x13e4136d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 환경 설정\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import StreamTableEnvironment, EnvironmentSettings\n",
    "\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "env.set_parallelism(3)\n",
    "env.add_jars(\"file:///Users/mzc01-hyucksangcho/Downloads/flink-sql-connector-kafka-3.3.0-1.19.jar\")\n",
    "\n",
    "settings = EnvironmentSettings.in_streaming_mode()\n",
    "t_env = StreamTableEnvironment.create(env, environment_settings=settings)\n",
    "t_env.get_config().set_local_timezone(\"Asia/Seoul\")\n",
    "t_env.get_config().get_configuration().set_string(\"execution.checkpointing.mode\", \"EXACTLY_ONCE\")\n",
    "t_env.get_config().get_configuration().set_string(\"execution.checkpointing.interval\", \"1 min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1d0574f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyflink.table.table_result.TableResult at 0x10d6afcd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Kafka 소스 테이블 생성\n",
    "\n",
    "t_env.execute_sql(\"\"\"\n",
    "CREATE TABLE transaction (\n",
    "    market STRING,\n",
    "    trade_date STRING,\n",
    "    trade_time STRING,\n",
    "    trade_date_kst STRING,\n",
    "    trade_time_kst STRING,\n",
    "    trade_timestamp BIGINT,\n",
    "    opening_price DOUBLE,\n",
    "    high_price DOUBLE,\n",
    "    low_price DOUBLE,\n",
    "    trade_price DOUBLE,\n",
    "    prev_closing_price DOUBLE,\n",
    "    change STRING,\n",
    "    change_price DOUBLE,\n",
    "    change_rate DOUBLE,\n",
    "    signed_change_price DOUBLE,\n",
    "    signed_change_rate DOUBLE,\n",
    "    trade_volume DOUBLE,\n",
    "    acc_trade_price DOUBLE,\n",
    "    acc_trade_price_24h DOUBLE,\n",
    "    acc_trade_volume DOUBLE,\n",
    "    acc_trade_volume_24h DOUBLE,\n",
    "    highest_52_week_price DOUBLE,\n",
    "    highest_52_week_date STRING,\n",
    "    lowest_52_week_price DOUBLE,\n",
    "    lowest_52_week_date STRING,\n",
    "    `timestamp` BIGINT,\n",
    "    event_time AS TO_TIMESTAMP_LTZ(`timestamp`, 3),\n",
    "    WATERMARK FOR event_time AS event_time - INTERVAL '1' SECOND\n",
    ") WITH (\n",
    "    'connector' = 'kafka',\n",
    "    'topic' = 'transction',\n",
    "    'properties.bootstrap.servers' = '43.201.105.43:9091',\n",
    "    'properties.group.id' = 'flink-group',\n",
    "    'scan.startup.mode' = 'latest-offset',\n",
    "    'format' = 'json',\n",
    "    'json.fail-on-missing-field' = 'false',\n",
    "    'json.ignore-parse-errors' = 'true'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# t_env.execute_sql(\"SELECT * FROM transaction\").print()\n",
    "# t_env.execute_sql(\"desc transaction\").print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "067430e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyflink.table.table_result.TableResult at 0x10d66c580>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 거래 금액 집계 뷰 생성 (1분 윈도우)\n",
    "# MINUTES\n",
    "\n",
    "t_env.execute_sql(\n",
    "\"\"\"\n",
    "CREATE TEMPORARY VIEW trade_volume_per_minute AS \n",
    "SELECT\n",
    "    market,\n",
    "    window_start, \n",
    "    window_end,\n",
    "    FLOOR(SUM(trade_price * trade_volume)) as one_m_trade_volume\n",
    "FROM TABLE(TUMBLE(TABLE transaction, DESCRIPTOR(event_time), INTERVAL '1' MINUTES))\n",
    "GROUP BY market, window_start, window_end\n",
    "\"\"\")\n",
    "\n",
    "#table_result1 = t_env.execute_sql(\"SELECT * FROM trade_volume_per_minute\")\n",
    "#table_result1.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02f0cd07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyflink.table.table_result.TableResult at 0x13e422d60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. 윈도우별 Top 5 추출\n",
    "\n",
    "t_env.execute_sql(\"\"\"\n",
    "CREATE TEMPORARY VIEW top5_trade_volume_per_minute AS\n",
    "SELECT *\n",
    "FROM (\n",
    "    SELECT *,\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY window_start, window_end \n",
    "            ORDER BY one_m_trade_volume DESC\n",
    "        ) AS row_num\n",
    "    FROM trade_volume_per_minute\n",
    ")\n",
    "WHERE row_num <= 5\n",
    "\"\"\")\n",
    "\n",
    "# t_env.execute_sql(\"SELECT * FROM top5_trade_volume_per_minute\").print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9f1fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-10 10:53:36,166 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig                [] - Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "2025-07-10 10:53:36,178 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl            [] - Scheduled Metric snapshot period at 10 second(s).\n",
      "2025-07-10 10:53:36,179 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl            [] - s3a-file-system metrics system started\n"
     ]
    }
   ],
   "source": [
    "# 5. S3 Sink 저장\n",
    "# Partition으로 컬럼을 사용하면 데이터에는 해당 컬럼이 저장안됨.\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "year = today.year\n",
    "month = today.month\n",
    "day = today.day\n",
    "\n",
    "t_env.execute_sql(f\"\"\"\n",
    "CREATE TABLE s3_sink (\n",
    "    market STRING,\n",
    "    window_start TIMESTAMP(3),\n",
    "    window_end TIMESTAMP(3),\n",
    "    one_m_trade_volume DOUBLE,\n",
    "    row_num BIGINT,\n",
    "    `year` STRING,\n",
    "    `month` STRING,\n",
    "    `day` STRING\n",
    "  ) PARTITIONED BY (`year`, `month`, `day`)\n",
    "  WITH (\n",
    "    'connector' = 'filesystem',\n",
    "    'path' = 's3a://pyflink-test-hs/trade_volume_per_minute/',\n",
    "    'sink.partition-commit.policy.kind'='success-file',\n",
    "    'format' = 'json'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "table_result = t_env.execute_sql(\"\"\"\n",
    "INSERT INTO s3_sink \n",
    "SELECT  \n",
    "      market,\n",
    "      window_start,\n",
    "      window_end,\n",
    "      one_m_trade_volume,\n",
    "      row_num,\n",
    "      DATE_FORMAT(window_start, 'yyyy') as `year`,\n",
    "      DATE_FORMAT(window_start, 'MM') as `month`,\n",
    "      DATE_FORMAT(window_start, 'dd') as `day`\n",
    "FROM top5_trade_volume_per_minute;\n",
    "\"\"\")\n",
    "\n",
    "table_result.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f12579b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyflink_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
